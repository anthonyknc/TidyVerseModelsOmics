---
title: "Supervised Machine Learning--Linear Regression"
author: Anthony Chan
format:
  html:    
    self-contained: true
toc: true
date: "2024-08-23"
---

```{r}
#| output: false
library(tidyverse)
library(tidymodels)
```

## Creating training and test datasets

The `rsample` package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.

`home_sales`: This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016. `selling_price` is the outcome variable.

```{r}
#| output: false
# Home sales dataset
home_sales <- read_csv("data/home_sales.csv")
```

```{r}
# Create a data split object
home_split <- initial_split(
  data = home_sales,
  prop = 0.70,
  strata = selling_price
)

# Create the training data
home_train <- home_split %>% training()

# Create the test data
home_test <- home_split %>% testing()

# Check number of rows in each dataset
nrow(home_train)
nrow(home_test)
```

## Distribution of outcome variable values
Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.
```{r}
# Distribution of selling_price in training data
home_train %>% 
  summarize(
    min_sell_price = min(selling_price),
    max_sell_price = max(selling_price),
    mean_sell_price = mean(selling_price),
    sd_sell_price = sd(selling_price)
  )

# Distribution of selling_price in test data
home_test  %>% 
  summarize(
    min_sell_price = min(selling_price),
    max_sell_price = max(selling_price),
    mean_sell_price = mean(selling_price),
    sd_sell_price = sd(selling_price)
  )
```

## Fitting a linear regression model
The parsnip package provides a unified syntax for the model fitting process in R.
With parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.

```{r}
# Initialize a linear regression object, linear_model
linear_model <- linear_reg() %>% 
  # Set the model engine
  set_engine("lm") %>% 
  # Set the model mode
  set_mode("regression")

# Fit the model using the training data
# Only use home_age and sqft_living as the predictor variables for now
lm_fit <- linear_model %>% 
  fit(selling_price ~ home_age + sqft_living, data = home_train)

# Print lm_fit to view model information
lm_fit %>% tidy()
```
The `tidy()` function automatically creates a tibble of estimated model parameters. Since `sqft_living` has a positive estimated parameter, the selling price of homes increases with the square footage. Conversely, since `home_age` has a negative estimated parameter, older homes tend to have lower selling prices.


## Predicting home selling prices
After fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.
Before you can evaluate model performance, you must add your predictions to the test dataset.

```{r}
# Predict selling_price
home_predictions <- predict(lm_fit, new_data = home_test)

# View predicted selling prices
home_predictions

# Combine test data with predictions
home_test_results <- home_test %>% 
  select(selling_price, home_age, sqft_living) %>% 
  bind_cols(home_predictions)

# View results
home_test_results
```


## Model performance metrics
Evaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.

```{r}
# Print home_test_results
home_test_results

# Calculate the RMSE metric
home_test_results %>% rmse(selling_price, .pred)

# Calculate the R squared metric
home_test_results %>% rsq(selling_price, .pred)
```

## R squared plot
Calculating the R squared value is only the first step in studying your model's predictions. Making an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.

```{r}
# Create an R squared plot of model performance
ggplot(home_test_results, aes(x = selling_price, y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "blue", linetype = 2) +
  # Use same scale for plots of observed vs predicted values
  coord_obs_pred() +
  labs(x = "Actual Home Selling Price", y = "Predicted Selling Price")
```

## Complete model fitting process with last_fit()

In this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using **all the predictors** available in the home_sales tibble. This exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.

```{r}
# Define a linear regression model
linear_model <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# Train linear_model with last_fit() using all available predictors
linear_fit <- linear_model %>% 
  last_fit(selling_price ~ ., split = home_split)

# Collect predictions and view results
predictions_df <- linear_fit %>% collect_predictions()
predictions_df

# Make an R squared plot using predictions_df
ggplot(predictions_df, aes(x = selling_price, y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "blue", linetype = 2) +
  coord_obs_pred() +
  labs(x = "Actual Home Selling Price", y = "Predicted Selling Price")
```
