---
title: "Supervised Machine Learning--Classification, Logistic Regression"
author: Anthony Chan
format:
  html:    
    self-contained: true
toc: true
date-modified: last-modified
date-format: "YYYY-MM-DD"
---

```{r}
#| output: false
library(tidyverse)
library(tidymodels)
library(here)

# Load the data
telecom_df <- read_csv(here("data", "telecom_df.csv")) %>%
  # Outcome variable needs to be a factor with first level being the positive class for classification
  mutate(canceled_service = factor(canceled_service, levels = c("yes", "no")))
```

## Data resampling
- Dataset: `telecom_df`
- Outcome variable: `canceled_service`
- Predictor variables: `avg_call_mins`,`avg_intl_mins`, and `monthly_charges`

```{r}
# Create data split object
telecom_split <- initial_split(
    data = telecom_df,
    prop = 0.75,
    strata = canceled_service
)

# Create the training data
telecom_train <- telecom_split %>% training()

# Create the test data
telecom_test <- telecom_split %>% testing()

# Check the number of rows
nrow(telecom_train)
nrow(telecom_test)
```

## Fitting a logistic regression model

```{r}
# Specify a logistic regression model
logistic_model <- logistic_reg() %>%
  # Set the engine
  set_engine("glm") %>%
  # Set the mode
  set_mode("classification")

# Print the model specification
logistic_model

# Fit to training data
logistic_fit <- logistic_model %>%
  fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges,
      data = telecom_train)

# Print model fit object
logistic_fit
```

## Combining test dataset results

```{r}
# Predict outcome categories
class_preds <- predict(logistic_fit, new_data = telecom_test, type = "class")

# Obtain estimated probabilities for each outcome value
prob_preds <- predict(logistic_fit, new_data = telecom_test, type = "prob")

# Combine test set results
telecom_results <- telecom_test %>% 
  select(canceled_service, avg_call_mins, avg_intl_mins, monthly_charges) %>% 
  bind_cols(class_preds, prob_preds)

# View results tibble
telecom_results
```

## Evaluating performance with yardstick

```{r}
# Calculate the confusion matrix
conf_mat(data = telecom_results, truth = canceled_service, estimate = .pred_class)

# Calculate the accuracy
accuracy(data = telecom_results, truth = canceled_service, estimate = .pred_class)

# Calculate the sensitivity
sens(data = telecom_results, truth = canceled_service, estimate = .pred_class)

# Calculate the specificity
spec(data = telecom_results, truth = canceled_service, estimate = .pred_class)
```

The specificity of your logistic regression model is 0.895, which is more than double the sensitivity of 0.42. This indicates that your model is much better at detecting customers who will not cancel their telecommunications service versus the ones who will.

## Creating custom metric sets

```{r}
# Create a custom metric function
telecom_metrics <- metric_set(accuracy, sens, spec)

# Calculate metrics using model results tibble
telecom_metrics(telecom_results, truth = canceled_service, estimate = .pred_class)

# Create a confusion matrix
conf_mat(
  data = telecom_results,
  truth = canceled_service,
  estimate = .pred_class
) %>%
  # Pass to the summary() function
  summary()
```

## Plotting the confusion matrix

```{r}
# Create a confusion matrix
conf_mat(data = telecom_results,
         truth = canceled_service,
         estimate = .pred_class) %>% 
  # Create a heat map
  autoplot(type = "heatmap")

# Create a confusion matrix
conf_mat(data = telecom_results,
         truth = canceled_service,
         estimate = .pred_class) %>% 
  # Create a mosaic plot
  autoplot(type = "mosaic")
```

## ROC curves and area under the ROC curve

ROC curves are used to visualize the performance of a classification model across a range of probability thresholds. The area under this curve provides a letter grade summary of model performance.

Check this video qout for detailed explanation: [ROC and AUC, Clearly Explained!](https://youtu.be/4jRBRDbJemM?si=uOlz5kUw-nyT2PF1) by Josh Starmer

```{r}
# Calculate metrics across thresholds
threshold_df <- telecom_results %>% roc_curve(truth = canceled_service, .pred_yes)

# View results
threshold_df

# Plot ROC curve
threshold_df %>% autoplot()

# Calculate ROC AUC
roc_auc(telecom_results, truth = canceled_service, .pred_yes)
```

## Streamlining the modeling process
The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.

```{r}
# Train model with last_fit()
telecom_last_fit <- logistic_model %>%
  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges,
    split = telecom_split
  )

# View test set metrics
telecom_last_fit %>% collect_metrics()

# Collect predictions
last_fit_results <- telecom_last_fit %>% collect_predictions()

# View results
last_fit_results

# Custom metrics function
last_fit_metrics <- metric_set(accuracy, sens, spec, roc_auc)


# Calculate metrics
last_fit_metrics(
  last_fit_results,
  truth = canceled_service,
  estimate = .pred_class,
  .pred_yes
)
```

## Complete modeling workflow
```{r}
# Train a logistic regression model
# With an additional predictor variable to see if you can improve model performance
logistic_fit <- logistic_model %>% 
  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, split = telecom_split)

# Collect metrics
logistic_fit %>% collect_metrics()

# Collect model predictions
logistic_fit %>% 
  collect_predictions() %>% 
  # Plot ROC curve
  roc_curve(truth = canceled_service, .pred_yes) %>% 
  autoplot()
```
The ROC curve shows that the logistic regression model performs better than a model that guesses at random (the dashed line in the plot). Adding the months_with_company predictor variable increased your area under the ROC curve from 0.76 in your previous model to 0.837!
